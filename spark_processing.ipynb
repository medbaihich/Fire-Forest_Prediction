{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2661ac3f-55f7-4859-9fe6-f3c98f1b779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Done.\n",
      "NULL date rows: 4698\n",
      "Fire aggregation completed.\n",
      "Distinct weather grid cells: 585\n",
      "JOIN Completed.\n",
      "\n",
      "=== SAMPLE ROWS ===\n",
      "+----------+--------+--------+----------+-------+-------+-------+\n",
      "| date_join|lat_grid|lon_grid|  temp_max|n_fires|sum_frp|is_fire|\n",
      "+----------+--------+--------+----------+-------+-------+-------+\n",
      "|2018-01-01|    42.0|    26.0| 12.369537|      0|    0.0|      0|\n",
      "|2018-01-02|    42.0|    26.0|11.1371155|      0|    0.0|      0|\n",
      "|2018-01-03|    42.0|    26.0|  8.972076|      0|    0.0|      0|\n",
      "|2018-01-04|    42.0|    26.0|  5.990387|      0|    0.0|      0|\n",
      "|2018-01-05|    42.0|    26.0| 10.610748|      0|    0.0|      0|\n",
      "|2018-01-06|    42.0|    26.0|12.7430725|      0|    0.0|      0|\n",
      "|2018-01-07|    42.0|    26.0| 11.988434|      0|    0.0|      0|\n",
      "|2018-01-08|    42.0|    26.0|  9.124176|      0|    0.0|      0|\n",
      "|2018-01-09|    42.0|    26.0| 6.8346252|      0|    0.0|      0|\n",
      "|2018-01-10|    42.0|    26.0|  7.244293|      0|    0.0|      0|\n",
      "|2018-01-11|    42.0|    26.0|  7.323639|      0|    0.0|      0|\n",
      "|2018-01-12|    42.0|    26.0|  7.848053|      0|    0.0|      0|\n",
      "|2018-01-13|    42.0|    26.0| 4.5414124|      0|    0.0|      0|\n",
      "|2018-01-14|    42.0|    26.0| 0.9847717|      0|    0.0|      0|\n",
      "|2018-01-15|    42.0|    26.0|  0.707428|      0|    0.0|      0|\n",
      "|2018-01-16|    42.0|    26.0| 3.6576233|      0|    0.0|      0|\n",
      "|2018-01-17|    42.0|    26.0| 10.335114|      0|    0.0|      0|\n",
      "|2018-01-18|    42.0|    26.0| 6.4476624|      0|    0.0|      0|\n",
      "|2018-01-19|    42.0|    26.0| 10.630768|      0|    0.0|      0|\n",
      "|2018-01-20|    42.0|    26.0|  8.950592|      0|    0.0|      0|\n",
      "+----------+--------+--------+----------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "=== FIRE DISTRIBUTION ===\n",
      "+-------+-------+\n",
      "|is_fire|  count|\n",
      "+-------+-------+\n",
      "|      1|   5230|\n",
      "|      0|6406370|\n",
      "+-------+-------+\n",
      "\n",
      "\n",
      "Pipeline completed. No file saved.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, round, lit, year, month, dayofmonth, count, sum as spark_sum, max as spark_max, avg\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ForestFire_Project\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://mongodb:27017/bigdata.batch_results\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# paths\n",
    "weather_path = \"hdfs://namenode:9000/raw_data/turkey_weather/era5_daily.csv\"\n",
    "fire_path = \"hdfs://namenode:9000/raw_data/turkey/*.csv\"\n",
    "\n",
    "# 1) read\n",
    "df_weather = spark.read.csv(weather_path, header=True, inferSchema=True)\n",
    "df_fire = spark.read.option(\"mergeSchema\",\"true\").csv(fire_path, header=True, inferSchema=True)\n",
    "\n",
    "print(\"Reading Done.\")\n",
    "\n",
    "# 2) CLEAN fire dates (remove invalid dates)\n",
    "df_fire = df_fire.withColumn(\"date_join\", to_date(col(\"acq_date\")))\n",
    "null_dates = df_fire.filter(col(\"date_join\").isNull()).count()\n",
    "print(\"NULL date rows:\", null_dates)\n",
    "\n",
    "df_fire = df_fire.filter(col(\"date_join\").isNotNull())\n",
    "\n",
    "# 3) create grid keys (0.1 degree grid)\n",
    "grid_step = 0.1\n",
    "df_fire = df_fire.withColumn(\"lat_grid\", (round(col(\"latitude\")/grid_step)*grid_step).cast(T.DoubleType())) \\\n",
    "                 .withColumn(\"lon_grid\", (round(col(\"longitude\")/grid_step)*grid_step).cast(T.DoubleType()))\n",
    "\n",
    "# 4) aggregate fire records per cell-day\n",
    "fire_agg = df_fire.groupBy(\"lat_grid\",\"lon_grid\",\"date_join\").agg(\n",
    "    count(\"*\").alias(\"n_fires\"),\n",
    "    spark_sum(\"frp\").alias(\"sum_frp\"),\n",
    "    spark_max(\"brightness\").alias(\"max_brightness\"),\n",
    "    avg(\"confidence\").alias(\"mean_confidence\"),\n",
    "    count( (col(\"daynight\") == \"D\").cast(\"int\") ).alias(\"count_day\"),\n",
    "    count( (col(\"daynight\") == \"N\").cast(\"int\") ).alias(\"count_night\")\n",
    ")\n",
    "\n",
    "print(\"Fire aggregation completed.\")\n",
    "\n",
    "# 5) prepare weather data\n",
    "df_weather_clean = df_weather.withColumnRenamed(\"latitude\",\"lat_grid\") \\\n",
    "                             .withColumnRenamed(\"longitude\",\"lon_grid\") \\\n",
    "                             .withColumn(\"date_join\", to_date(col(\"time\"))) \\\n",
    "                             .withColumn(\"lat_grid\", col(\"lat_grid\").cast(T.DoubleType())) \\\n",
    "                             .withColumn(\"lon_grid\", col(\"lon_grid\").cast(T.DoubleType()))\n",
    "\n",
    "print(\"Distinct weather grid cells:\",\n",
    "      df_weather_clean.select(\"lat_grid\",\"lon_grid\").distinct().count())\n",
    "\n",
    "# 6) join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 20 * 1024 * 1024)\n",
    "\n",
    "df_integrated = df_weather_clean.join(\n",
    "    broadcast(fire_agg),\n",
    "    on=[\"lat_grid\",\"lon_grid\",\"date_join\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"JOIN Completed.\")\n",
    "\n",
    "# 7) fill missing values + add date features\n",
    "df_final = df_integrated.fillna({\n",
    "    'n_fires':0,\n",
    "    'sum_frp':0.0,\n",
    "    'max_brightness':0.0,\n",
    "    'mean_confidence':0.0,\n",
    "    'count_day':0,\n",
    "    'count_night':0\n",
    "}).withColumn(\"is_fire\", (col(\"n_fires\") > 0).cast(\"int\")) \\\n",
    "  .withColumn(\"year\", year(col(\"date_join\"))) \\\n",
    "  .withColumn(\"month\", month(col(\"date_join\"))) \\\n",
    "  .withColumn(\"day\", dayofmonth(col(\"date_join\")))\n",
    "\n",
    "# 8) Show sample\n",
    "print(\"\\n=== SAMPLE ROWS ===\")\n",
    "df_final.select(\"date_join\",\"lat_grid\",\"lon_grid\",\"temp_max\",\"n_fires\",\"sum_frp\",\"is_fire\").show(20)\n",
    "\n",
    "# 9) Show fire distribution\n",
    "print(\"\\n=== FIRE DISTRIBUTION ===\")\n",
    "df_final.groupBy(\"is_fire\").count().show()\n",
    "\n",
    "print(\"\\nPipeline completed. No file saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653a36f3-3c27-4d35-b00f-9becbf0e0ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 6554130\n"
     ]
    }
   ],
   "source": [
    "# Print total number of rows for debugging (optional)\n",
    "print(f\"Total Rows: {df_integrated.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70523b48-ce3e-4824-9eae-f4db2ff47259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) SAVE FULL checkpoint ----------\n",
    "print(f\"Saving FULL dataset to {full_output_path} ...\")\n",
    "df_final.write.mode(\"overwrite\").parquet(full_output_path)\n",
    "print(\"Full dataset saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0c1b0a-d22c-4cbc-868b-a25e415b18b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts -> pos: 5230, neg: 6406370\n",
      "Sampling fraction for negative class (capped to 1.0): 0.002449\n",
      "\n",
      "=== BALANCED DISTRIBUTION ===\n",
      "+-------+-----+\n",
      "|is_fire|count|\n",
      "+-------+-----+\n",
      "|      1| 5230|\n",
      "|      0|15563|\n",
      "+-------+-----+\n",
      "\n",
      "\n",
      "Sample rows:\n",
      "+----------+--------+--------+---------+-------+------------------+-------+\n",
      "|date_join |lat_grid|lon_grid|temp_max |n_fires|sum_frp           |is_fire|\n",
      "+----------+--------+--------+---------+-------+------------------+-------+\n",
      "|2018-10-16|38.0    |41.5    |28.173492|2      |40.7              |1      |\n",
      "|2020-03-12|42.0    |29.0    |10.052887|0      |0.0               |0      |\n",
      "|2019-02-21|37.0    |34.5    |11.490631|0      |0.0               |0      |\n",
      "|2020-06-21|36.0    |44.5    |31.717194|0      |0.0               |0      |\n",
      "|2020-09-03|41.0    |45.0    |20.95108 |0      |0.0               |0      |\n",
      "|2020-11-10|38.0    |33.5    |16.048492|2      |14.899999999999999|1      |\n",
      "|2018-07-15|42.0    |44.0    |21.86734 |0      |0.0               |0      |\n",
      "|2020-12-16|38.0    |28.0    |15.914215|0      |0.0               |0      |\n",
      "|2019-12-26|41.5    |41.0    |12.258942|0      |0.0               |0      |\n",
      "|2019-10-24|40.5    |39.5    |15.491364|0      |0.0               |0      |\n",
      "+----------+--------+--------+---------+-------+------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# ---------- 0) params ----------\n",
    "full_output_path = \"hdfs://namenode:9000/processed_data/turkey_integrated_full\"\n",
    "train_output_path = \"hdfs://namenode:9000/processed_data/turkey_training_ready\"\n",
    "neg_ratio = 3            # desired ratio: negatives : positives = 3 : 1 (i.e., non-fire = 3 * fire)\n",
    "rnd_seed = 42\n",
    "shuffle_partitions = 200  # adjust according to your cluster\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 2) cache before expensive counts ----------\n",
    "# Repartition to distribute data, then persist to avoid repeated computation for counts and sampling\n",
    "df_final = df_final.repartition(shuffle_partitions).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "pos_count = df_final.filter(col(\"is_fire\") == 1).count()\n",
    "neg_count = df_final.filter(col(\"is_fire\") == 0).count()\n",
    "print(f\"Counts -> pos: {pos_count}, neg: {neg_count}\")\n",
    "\n",
    "# ---------- 3) determine sampling fraction safely ----------\n",
    "target_neg_count = pos_count * neg_ratio\n",
    "if neg_count == 0:\n",
    "    raise ValueError(\"No negative rows found â€” check df_final!\")\n",
    "fraction = target_neg_count / float(neg_count)\n",
    "fraction = min(1.0, fraction)  # ensure <= 1.0 for sampling without replacement\n",
    "print(f\"Sampling fraction for negative class (capped to 1.0): {fraction:.6f}\")\n",
    "\n",
    "# ---------- 4) undersample negatives ----------\n",
    "df_pos = df_final.filter(col(\"is_fire\") == 1)\n",
    "df_neg = df_final.filter(col(\"is_fire\") == 0)\n",
    "df_neg_sampled = df_neg.sample(withReplacement=False, fraction=fraction, seed=rnd_seed)\n",
    "\n",
    "# ---------- 5) union (balanced) ----------\n",
    "df_balanced = df_pos.unionByName(df_neg_sampled)\n",
    "\n",
    "# ---------- 6) optional: light shuffle (avoid heavy global orderBy) ----------\n",
    "# Approach: add a random column, repartition by it, then sort within partitions.\n",
    "# This is cheaper than a full global orderBy(rand()) while providing a good shuffle.\n",
    "from pyspark.sql.functions import rand as spark_rand\n",
    "df_balanced = df_balanced.withColumn(\"_rand\", spark_rand(seed=rnd_seed)) \\\n",
    "                         .repartition(shuffle_partitions, \"_rand\") \\\n",
    "                         .sortWithinPartitions(\"_rand\") \\\n",
    "                         .drop(\"_rand\")\n",
    "\n",
    "# ---------- 7) quick checks ----------\n",
    "print(\"\\n=== BALANCED DISTRIBUTION ===\")\n",
    "df_balanced.groupBy(\"is_fire\").count().show()\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "df_balanced.select(\"date_join\",\"lat_grid\",\"lon_grid\",\"temp_max\",\"n_fires\",\"sum_frp\",\"is_fire\").show(10, truncate=False)\n",
    "\n",
    "# ---------- 8) Save READY-TO-TRAIN (only if you confirm) ----------\n",
    "# Uncomment below to persist the balanced dataset to HDFS\n",
    "# df_balanced.write.mode(\"overwrite\").parquet(train_output_path)\n",
    "# print(\"Saved balanced training dataset to:\", train_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21d0d64-d27b-498e-9dca-93028b99ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved balanced training dataset to: hdfs://namenode:9000/processed_data/turkey_training_ready\n"
     ]
    }
   ],
   "source": [
    "# ---------- 8) Save READY-TO-TRAIN ----------\n",
    "df_balanced.write.mode(\"overwrite\").parquet(train_output_path)\n",
    "print(\"Saved balanced training dataset to:\", train_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31c62cd-82c6-45b6-b3f7-a5103eb22ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Loaded: 20793\n",
      "Training Set (2018-2019): 13500\n",
      "Test Set (2020): 7293\n",
      "Using features (SAFE LIST): ['lat_grid', 'lon_grid', 'month', 'day', 'temp_mean', 'temp_max', 'temp_min', 'humidity_mean', 'humidity_min', 'wind_speed_mean', 'wind_speed_max']\n",
      "Sizes: train= 13500 test= 7293\n",
      "Training Model...\n",
      "Predicting...\n",
      "\n",
      "=== FINAL RESULTS (Test 2020) ===\n",
      "Accuracy: 0.8553\n",
      "F1 Score: 0.8451\n",
      "ROC AUC:  0.9285\n",
      "\n",
      "Feature Importances:\n",
      " - lat_grid: 0.2240\n",
      " - lon_grid: 0.1368\n",
      " - month: 0.1059\n",
      " - day: 0.0416\n",
      " - temp_mean: 0.0520\n",
      " - temp_max: 0.0597\n",
      " - temp_min: 0.0393\n",
      " - humidity_mean: 0.0917\n",
      " - humidity_min: 0.1805\n",
      " - wind_speed_mean: 0.0396\n",
      " - wind_speed_max: 0.0290\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ForestFire_ML_Training\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Read the prepared dataset (Balanced Data)\n",
    "train_path = \"hdfs://namenode:9000/processed_data/turkey_training_ready\"\n",
    "df = spark.read.parquet(train_path)\n",
    "\n",
    "print(f\"Total Data Loaded: {df.count()}\")\n",
    "\n",
    "# 3. Temporal Split\n",
    "# Train: before 2020\n",
    "# Test: 2020 and after\n",
    "train_data = df.filter(col(\"year\") < 2020)\n",
    "test_data = df.filter(col(\"year\") >= 2020)\n",
    "\n",
    "print(f\"Training Set (2018-2019): {train_data.count()}\")\n",
    "print(f\"Test Set (2020): {test_data.count()}\")\n",
    "\n",
    "# ... (same initialization and loading code)\n",
    "\n",
    "# ---------- features (very important modification) ----------\n",
    "# We removed n_fires, frp, brightness, confidence because they cause data leakage\n",
    "feature_cols = [\n",
    "    \"lat_grid\", \"lon_grid\", \"month\", \"day\",  # location and time\n",
    "    \"temp_mean\", \"temp_max\", \"temp_min\",     # temperature\n",
    "    \"humidity_mean\", \"humidity_min\",         # humidity (very important)\n",
    "    \"wind_speed_mean\", \"wind_speed_max\"      # wind\n",
    "]\n",
    "\n",
    "# Ensure the columns exist\n",
    "present_features = [c for c in feature_cols if c in df_balanced.columns]\n",
    "print(\"Using features (SAFE LIST):\", present_features)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=present_features, outputCol=\"features\", handleInvalid=\"skip\")  # skip is better than keep to avoid invalid values\n",
    "df_ready = assembler.transform(df_balanced).select(\"features\", \"is_fire\", \"date_join\", \"year\")  # ensure 'year' exists\n",
    "\n",
    "# ---------- split strategy (date correction) ----------\n",
    "# Train: 2018 + 2019\n",
    "# Test: 2020 (because you don't have 2021)\n",
    "train = df_ready.filter(col(\"year\") < 2020)\n",
    "test  = df_ready.filter(col(\"year\") == 2020)\n",
    "\n",
    "print(\"Sizes: train=\", train.count(), \"test=\", test.count())\n",
    "\n",
    "if test.count() == 0:\n",
    "    raise RuntimeError(\"Test set is empty! Check your year column or dataset.\")\n",
    "\n",
    "# ---------- training ----------\n",
    "# Increased number of trees slightly to improve accuracy\n",
    "rf = RandomForestClassifier(labelCol=\"is_fire\", featuresCol=\"features\", numTrees=100, maxDepth=10, seed=42)\n",
    "print(\"Training Model...\")\n",
    "model = rf.fit(train)\n",
    "\n",
    "# ---------- evaluation ----------\n",
    "print(\"Predicting...\")\n",
    "preds = model.transform(test)\n",
    "\n",
    "# accuracy & f1\n",
    "mcc = MulticlassClassificationEvaluator(labelCol=\"is_fire\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acc = mcc.evaluate(preds)\n",
    "mcc_f1 = MulticlassClassificationEvaluator(labelCol=\"is_fire\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = mcc_f1.evaluate(preds)\n",
    "\n",
    "# ROC & PR AUC\n",
    "bce_roc = BinaryClassificationEvaluator(labelCol=\"is_fire\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = bce_roc.evaluate(preds)\n",
    "\n",
    "print(f\"\\n=== FINAL RESULTS (Test 2020) ===\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC:  {roc_auc:.4f}\")\n",
    "\n",
    "# Feature Importance (to see what actually influences fires)\n",
    "rfModel = model\n",
    "print(\"\\nFeature Importances:\")\n",
    "importances = rfModel.featureImportances\n",
    "for i, imp in enumerate(importances):\n",
    "    if i < len(present_features):\n",
    "        print(f\" - {present_features[i]}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e056295-8e72-43e0-8f52-056c1975f7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at: hdfs://namenode:9000/models/turkey_fire_model_v1\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"hdfs://namenode:9000/models/turkey_fire_model_v1\"\n",
    "\n",
    "model.write().overwrite().save(model_save_path)\n",
    "\n",
    "print(\"Model saved successfully at:\", model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944284a3-fe95-4ebc-9da5-c06e840126e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
